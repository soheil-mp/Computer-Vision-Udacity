{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference and Validation\n",
    "\n",
    "Now that you have a trained network, you can use it for making predictions. This is typically called **inference**, a term borrowed from statistics. However, neural networks have a tendency to perform *too well* on the training data and aren't able to generalize to data that hasn't been seen before. This is called **overfitting** and it impairs inference performance. To test for overfitting while training, we measure the performance on data not in the training set called the **validation** dataset. We avoid overfitting through regularization such as dropout while monitoring the validation performance during training. In this notebook, I'll show you how to do this in PyTorch. \n",
    "\n",
    "First off, I'll implement my own feedforward network for the exercise you worked on in part 4 using the Fashion-MNIST dataset. This will serve as a solution for part 4, as well as an example of how to use dropout and validation.\n",
    "\n",
    "As usual, let's start by loading the dataset through torchvision. You'll learn more about torchvision and loading data in a later part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import helper\n",
    "import time\n",
    "\n",
    "# Importing the torch libraries\n",
    "import torch \n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define transform to normalize the data\n",
    "transform = transforms.Compose(transforms = [transforms.ToTensor(),\n",
    "                                             transforms.Normalize(mean = (0.5, 0.5, 0.5),\n",
    "                                                                  std = (0.5, 0.5, 0.5))])\n",
    "\n",
    "# Downloading and loading the training data\n",
    "trainset = datasets.FashionMNIST(root = './F_MNIST_data/', train = True, transform = transform, download = True)\n",
    "trainloader = torch.utils.data.DataLoader(dataset = trainset, batch_size = 64, shuffle = True)\n",
    "\n",
    "# Downliading and loading the test data\n",
    "testset = datasets.FashionMNIST(root = './F_MNIST_data/', train = False, transform = transform, download = True)\n",
    "testloader = torch.utils.data.DataLoader(dataset = testset, batch_size = 64, shuffle = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the network\n",
    "\n",
    "As with MNIST, each image in Fashion-MNIST is 28x28 which is a total of 784 pixels, and there are 10 classes. I used `nn.ModuleList` to allow for an arbitrary number of hidden layers. This model has an argument `hidden_layers` that's a list of the hidden layer sizes (as integers). Using `nn.ModuleList` registers each hidden layer module properly so you can use module methods in the model.\n",
    "\n",
    "I also have the `forward` method returning the log-softmax for the output. Since softmax is a probability distibution over the classes, the log-softmax is a log probability which comes with a [lot of benefits](https://en.wikipedia.org/wiki/Log_probability). Using the log probability, computations are often faster and more accurate. To get the class probabilities later, I'll need to take the exponential (`torch.exp`) of the output.\n",
    "\n",
    "We can include dropout in our network with [`nn.Dropout`](http://pytorch.org/docs/master/nn.html#dropout). This works similar to other modules such as `nn.Linear`. It also takes the dropout probability as an input which we can pass as an input to the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "    def __init__(self, input_size, output_size, hidden_layers, drop_p = 0.5):\n",
    "        ''' Builds a feedforward network with arbitrary hidden layers.\n",
    "        \n",
    "            Arguments\n",
    "            ---------\n",
    "            input_size: integer, size of the input\n",
    "            output_size: integer, size of the output layer\n",
    "            hidden_layers: list of integers, the sizes of the hidden layers\n",
    "            drop_p: float between 0 and 1, dropout probability\n",
    "        '''\n",
    "        super().__init__()\n",
    "        \n",
    "        # Adding the first layer (input to hidden layer)\n",
    "        self.hidden_layers = nn.ModuleList(modules = [nn.Linear(in_features = input_size, \n",
    "                                                                out_features = hidden_layers[0])])\n",
    "        \n",
    "        # Adding a variable number of more hidden layers\n",
    "        layer_sizes = zip(hidden_layers[:-1], hidden_layers[1:])\n",
    "        self.hidden_layers.extend([nn.Linear(in_features = h1, out_features = h2) for h1, h2 in layer_sizes])\n",
    "        \n",
    "        # Adding the output layer\n",
    "        self.output = nn.Linear(in_features = hidden_layers[-1], out_features = output_size)\n",
    "        \n",
    "        # Adding the dropout\n",
    "        self.dropout = nn.Dropout(p=drop_p)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        ''' Forward pass through the network, returns the output logits '''\n",
    "        \n",
    "        # Adding activation functiona and dropout to each hidden layer\n",
    "        for each in self.hidden_layers:\n",
    "            x = F.relu(each(x))\n",
    "            x = self.dropout(x)\n",
    "            \n",
    "        # Output logits\n",
    "        x = self.output(x)\n",
    "        \n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the network\n",
    "\n",
    "Since the model's forward method returns the log-softmax, I used the [negative log loss](http://pytorch.org/docs/master/nn.html#nllloss) as my criterion, `nn.NLLLoss()`. I also chose to use the [Adam optimizer](http://pytorch.org/docs/master/optim.html#torch.optim.Adam). This is a variant of stochastic gradient descent which includes momentum and in general trains faster than your basic SGD.\n",
    "\n",
    "I've also included a block to measure the validation loss and accuracy. Since I'm using dropout in the network, I need to turn it off during inference. Otherwise, the network will appear to perform poorly because many of the connections are turned off. PyTorch allows you to set a model in \"training\" or \"evaluation\" modes with `model.train()` and `model.eval()`, respectively. In training mode, dropout is turned on, while in evaluation mode, dropout is turned off. This effects other modules as well that should be on during training but off during inference.\n",
    "\n",
    "The validation code consists of a forward pass through the validation set (also split into batches). With the log-softmax output, I calculate the loss on the validation set, as well as the prediction accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the network, define the criterion and optimizer\n",
    "model = Network(784, 10, [500], drop_p=0.5)\n",
    "criterion = nn.NLLLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/soheil/anaconda3/lib/python3.6/site-packages/ipykernel/__main__.py:53: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "/Users/soheil/anaconda3/lib/python3.6/site-packages/ipykernel/__main__.py:54: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/3, Training Loss: 0.368, Test Loss: 0.403, Test Accuracy: 0.853\n",
      "Epoch: 1/3, Training Loss: 0.363, Test Loss: 0.388, Test Accuracy: 0.856\n",
      "Epoch: 1/3, Training Loss: 0.332, Test Loss: 0.378, Test Accuracy: 0.864\n",
      "Epoch: 1/3, Training Loss: 0.361, Test Loss: 0.383, Test Accuracy: 0.860\n",
      "Epoch: 1/3, Training Loss: 0.343, Test Loss: 0.378, Test Accuracy: 0.866\n",
      "Epoch: 1/3, Training Loss: 0.349, Test Loss: 0.394, Test Accuracy: 0.858\n",
      "Epoch: 1/3, Training Loss: 0.323, Test Loss: 0.433, Test Accuracy: 0.842\n",
      "Epoch: 1/3, Training Loss: 0.358, Test Loss: 0.377, Test Accuracy: 0.861\n",
      "Epoch: 1/3, Training Loss: 0.346, Test Loss: 0.367, Test Accuracy: 0.868\n",
      "Epoch: 2/3, Training Loss: 0.335, Test Loss: 0.392, Test Accuracy: 0.860\n",
      "Epoch: 2/3, Training Loss: 0.321, Test Loss: 0.399, Test Accuracy: 0.853\n",
      "Epoch: 2/3, Training Loss: 0.301, Test Loss: 0.382, Test Accuracy: 0.861\n",
      "Epoch: 2/3, Training Loss: 0.323, Test Loss: 0.371, Test Accuracy: 0.866\n",
      "Epoch: 2/3, Training Loss: 0.318, Test Loss: 0.367, Test Accuracy: 0.868\n",
      "Epoch: 2/3, Training Loss: 0.314, Test Loss: 0.374, Test Accuracy: 0.867\n",
      "Epoch: 2/3, Training Loss: 0.318, Test Loss: 0.371, Test Accuracy: 0.865\n",
      "Epoch: 2/3, Training Loss: 0.331, Test Loss: 0.362, Test Accuracy: 0.866\n",
      "Epoch: 2/3, Training Loss: 0.318, Test Loss: 0.353, Test Accuracy: 0.872\n",
      "Epoch: 3/3, Training Loss: 0.288, Test Loss: 0.374, Test Accuracy: 0.867\n",
      "Epoch: 3/3, Training Loss: 0.298, Test Loss: 0.376, Test Accuracy: 0.866\n",
      "Epoch: 3/3, Training Loss: 0.293, Test Loss: 0.378, Test Accuracy: 0.866\n",
      "Epoch: 3/3, Training Loss: 0.288, Test Loss: 0.371, Test Accuracy: 0.869\n",
      "Epoch: 3/3, Training Loss: 0.303, Test Loss: 0.372, Test Accuracy: 0.864\n",
      "Epoch: 3/3, Training Loss: 0.288, Test Loss: 0.351, Test Accuracy: 0.872\n",
      "Epoch: 3/3, Training Loss: 0.290, Test Loss: 0.357, Test Accuracy: 0.872\n",
      "Epoch: 3/3, Training Loss: 0.307, Test Loss: 0.361, Test Accuracy: 0.873\n",
      "Epoch: 3/3, Training Loss: 0.294, Test Loss: 0.354, Test Accuracy: 0.874\n",
      "Epoch: 3/3, Training Loss: 0.285, Test Loss: 0.350, Test Accuracy: 0.873\n"
     ]
    }
   ],
   "source": [
    "\"\"\"My version\"\"\"\n",
    "\n",
    "# Some initialization\n",
    "epochs = 3\n",
    "print_every = 100\n",
    "steps = 0\n",
    "running_loss = 0\n",
    "\n",
    "# Iteratingh through epochs \n",
    "for e in range(epochs):\n",
    "    \n",
    "    # Iterating through images and its labels\n",
    "    for images, labels in iter(trainloader):\n",
    "        \n",
    "        steps += 1\n",
    "        \n",
    "        # Flattening the image\n",
    "        #print(\"Image size before resizing: \", images.size())\n",
    "        images.resize_(images.size()[0], 28*28)\n",
    "        #print(\"Image size after resizing: \", images.size())\n",
    "        \n",
    "        # Wrap images and labels into a Variable\n",
    "        inputs = Variable(images)\n",
    "        targets = Variable(labels)\n",
    "        \n",
    "        # Restarting the gradients to zero\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Running the feedforward and backward propagation\n",
    "        output = model.forward(inputs)\n",
    "        loss = criterion(output, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Accumalating the loss\n",
    "        running_loss += loss\n",
    "        \n",
    "        # Getting the testing accuracy and loss\n",
    "        if steps % print_every == 0:\n",
    "            \n",
    "            # Turning the dropout off but activating the evaluation mode\n",
    "            model.eval()\n",
    "            \n",
    "            # Initialization\n",
    "            accuracy = 0\n",
    "            test_loss = 0\n",
    "            \n",
    "            # Iterating through the test images and its labels\n",
    "            for index, (images_test, labels_test) in enumerate(testloader):\n",
    "                \n",
    "                # Resizing the images\n",
    "                images_test.resize_(images_test.size()[0], 28*28)\n",
    "                \n",
    "                # Wrapping the images and labels into a variable\n",
    "                inputs_test = Variable(images_test, volatile = True)\n",
    "                targets_test = Variable(labels_test, volatile = True)\n",
    "                \n",
    "                # Forward pass\n",
    "                output_test = model.forward(inputs_test)\n",
    "                test_loss += criterion(output_test, targets_test)\n",
    "                \n",
    "                # Calculating the accuracy\n",
    "                ps = torch.exp(output_test).data\n",
    "                equality = (labels_test.data == ps.max(1)[1]) # choosing with highets probability\n",
    "                accuracy += equality.type_as(torch.FloatTensor()).mean()\n",
    "                \n",
    "            print(\"Epoch: {}/{}, Training Loss: {:.3f}, Test Loss: {:.3f}, Test Accuracy: {:.3f}\".format \\\n",
    "                  (e + 1, epochs, running_loss / print_every, test_loss / len(testloader), accuracy / len(testloader)))\n",
    "            \n",
    "            running_loss = 0\n",
    "            \n",
    "            # Make the dropout on by activating the train mode\n",
    "            model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/soheil/anaconda3/lib/python3.6/site-packages/ipykernel/__main__.py:23: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
      "/Users/soheil/anaconda3/lib/python3.6/site-packages/ipykernel/__main__.py:35: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "/Users/soheil/anaconda3/lib/python3.6/site-packages/ipykernel/__main__.py:36: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "/Users/soheil/anaconda3/lib/python3.6/site-packages/ipykernel/__main__.py:39: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/2..  Training Loss: 1.021..  Test Loss: 0.687..  Test Accuracy: 0.751\n",
      "Epoch: 1/2..  Training Loss: 0.723..  Test Loss: 0.613..  Test Accuracy: 0.780\n",
      "Epoch: 1/2..  Training Loss: 0.673..  Test Loss: 0.586..  Test Accuracy: 0.784\n",
      "Epoch: 1/2..  Training Loss: 0.647..  Test Loss: 0.564..  Test Accuracy: 0.789\n",
      "Epoch: 1/2..  Training Loss: 0.574..  Test Loss: 0.531..  Test Accuracy: 0.810\n",
      "Epoch: 1/2..  Training Loss: 0.559..  Test Loss: 0.519..  Test Accuracy: 0.808\n",
      "Epoch: 1/2..  Training Loss: 0.558..  Test Loss: 0.524..  Test Accuracy: 0.804\n",
      "Epoch: 1/2..  Training Loss: 0.561..  Test Loss: 0.497..  Test Accuracy: 0.819\n",
      "Epoch: 1/2..  Training Loss: 0.517..  Test Loss: 0.513..  Test Accuracy: 0.808\n",
      "Epoch: 1/2..  Training Loss: 0.540..  Test Loss: 0.487..  Test Accuracy: 0.820\n",
      "Epoch: 1/2..  Training Loss: 0.532..  Test Loss: 0.488..  Test Accuracy: 0.824\n",
      "Epoch: 1/2..  Training Loss: 0.532..  Test Loss: 0.492..  Test Accuracy: 0.822\n",
      "Epoch: 1/2..  Training Loss: 0.543..  Test Loss: 0.494..  Test Accuracy: 0.819\n",
      "Epoch: 1/2..  Training Loss: 0.483..  Test Loss: 0.482..  Test Accuracy: 0.823\n",
      "Epoch: 1/2..  Training Loss: 0.503..  Test Loss: 0.480..  Test Accuracy: 0.827\n",
      "Epoch: 1/2..  Training Loss: 0.501..  Test Loss: 0.473..  Test Accuracy: 0.824\n",
      "Epoch: 1/2..  Training Loss: 0.479..  Test Loss: 0.470..  Test Accuracy: 0.826\n",
      "Epoch: 1/2..  Training Loss: 0.460..  Test Loss: 0.500..  Test Accuracy: 0.815\n",
      "Epoch: 1/2..  Training Loss: 0.479..  Test Loss: 0.452..  Test Accuracy: 0.839\n",
      "Epoch: 1/2..  Training Loss: 0.499..  Test Loss: 0.465..  Test Accuracy: 0.828\n",
      "Epoch: 1/2..  Training Loss: 0.500..  Test Loss: 0.460..  Test Accuracy: 0.828\n",
      "Epoch: 1/2..  Training Loss: 0.475..  Test Loss: 0.456..  Test Accuracy: 0.833\n",
      "Epoch: 1/2..  Training Loss: 0.478..  Test Loss: 0.445..  Test Accuracy: 0.835\n",
      "Epoch: 2/2..  Training Loss: 0.460..  Test Loss: 0.455..  Test Accuracy: 0.835\n",
      "Epoch: 2/2..  Training Loss: 0.480..  Test Loss: 0.440..  Test Accuracy: 0.838\n",
      "Epoch: 2/2..  Training Loss: 0.462..  Test Loss: 0.447..  Test Accuracy: 0.836\n",
      "Epoch: 2/2..  Training Loss: 0.484..  Test Loss: 0.439..  Test Accuracy: 0.840\n",
      "Epoch: 2/2..  Training Loss: 0.488..  Test Loss: 0.444..  Test Accuracy: 0.838\n",
      "Epoch: 2/2..  Training Loss: 0.418..  Test Loss: 0.429..  Test Accuracy: 0.844\n",
      "Epoch: 2/2..  Training Loss: 0.447..  Test Loss: 0.452..  Test Accuracy: 0.836\n",
      "Epoch: 2/2..  Training Loss: 0.450..  Test Loss: 0.443..  Test Accuracy: 0.840\n",
      "Epoch: 2/2..  Training Loss: 0.470..  Test Loss: 0.427..  Test Accuracy: 0.848\n",
      "Epoch: 2/2..  Training Loss: 0.425..  Test Loss: 0.427..  Test Accuracy: 0.847\n",
      "Epoch: 2/2..  Training Loss: 0.467..  Test Loss: 0.420..  Test Accuracy: 0.853\n",
      "Epoch: 2/2..  Training Loss: 0.497..  Test Loss: 0.418..  Test Accuracy: 0.846\n",
      "Epoch: 2/2..  Training Loss: 0.447..  Test Loss: 0.419..  Test Accuracy: 0.847\n",
      "Epoch: 2/2..  Training Loss: 0.438..  Test Loss: 0.421..  Test Accuracy: 0.847\n",
      "Epoch: 2/2..  Training Loss: 0.445..  Test Loss: 0.432..  Test Accuracy: 0.840\n",
      "Epoch: 2/2..  Training Loss: 0.469..  Test Loss: 0.421..  Test Accuracy: 0.848\n",
      "Epoch: 2/2..  Training Loss: 0.442..  Test Loss: 0.416..  Test Accuracy: 0.847\n",
      "Epoch: 2/2..  Training Loss: 0.450..  Test Loss: 0.427..  Test Accuracy: 0.845\n",
      "Epoch: 2/2..  Training Loss: 0.432..  Test Loss: 0.430..  Test Accuracy: 0.846\n",
      "Epoch: 2/2..  Training Loss: 0.428..  Test Loss: 0.427..  Test Accuracy: 0.841\n",
      "Epoch: 2/2..  Training Loss: 0.448..  Test Loss: 0.417..  Test Accuracy: 0.846\n",
      "Epoch: 2/2..  Training Loss: 0.408..  Test Loss: 0.412..  Test Accuracy: 0.844\n",
      "Epoch: 2/2..  Training Loss: 0.419..  Test Loss: 0.408..  Test Accuracy: 0.851\n"
     ]
    }
   ],
   "source": [
    "epochs = 2\n",
    "steps = 0\n",
    "running_loss = 0\n",
    "print_every = 40\n",
    "for e in range(epochs):\n",
    "    # Model in training mode, dropout is on\n",
    "    model.train()\n",
    "    for images, labels in iter(trainloader):\n",
    "        steps += 1\n",
    "        # Flatten images into a 784 long vector\n",
    "        images.resize_(images.size()[0], 784)\n",
    "        \n",
    "        # Wrap images and labels in Variables so we can calculate gradients\n",
    "        inputs = Variable(images)\n",
    "        targets = Variable(labels)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output = model.forward(inputs)\n",
    "        loss = criterion(output, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.data[0]\n",
    "        \n",
    "        if steps % print_every == 0:\n",
    "            # Model in inference mode, dropout is off\n",
    "            model.eval()\n",
    "            \n",
    "            accuracy = 0\n",
    "            test_loss = 0\n",
    "            for ii, (images, labels) in enumerate(testloader):\n",
    "                \n",
    "                images = images.resize_(images.size()[0], 784)\n",
    "                # Set volatile to True so we don't save the history\n",
    "                inputs = Variable(images, volatile=True)\n",
    "                labels = Variable(labels, volatile=True)\n",
    "\n",
    "                output = model.forward(inputs)\n",
    "                test_loss += criterion(output, labels).data[0]\n",
    "                \n",
    "                ## Calculating the accuracy \n",
    "                # Model's output is log-softmax, take exponential to get the probabilities\n",
    "                ps = torch.exp(output).data\n",
    "                # Class with highest probability is our predicted class, compare with true label\n",
    "                equality = (labels.data == ps.max(1)[1])\n",
    "                # Accuracy is number of correct predictions divided by all predictions, just take the mean\n",
    "                accuracy += equality.type_as(torch.FloatTensor()).mean()\n",
    "            \n",
    "            print(\"Epoch: {}/{}.. \".format(e+1, epochs),\n",
    "                  \"Training Loss: {:.3f}.. \".format(running_loss/print_every),\n",
    "                  \"Test Loss: {:.3f}.. \".format(test_loss/len(testloader)),\n",
    "                  \"Test Accuracy: {:.3f}\".format(accuracy/len(testloader)))\n",
    "            \n",
    "            running_loss = 0\n",
    "            \n",
    "            # Make sure dropout is on for training\n",
    "            model.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference\n",
    "\n",
    "Now that the model is trained, we can use it for inference. We've done this before, but now we need to remember to set the model in inference mode with `model.eval()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/soheil/anaconda3/lib/python3.6/site-packages/ipykernel/__main__.py:12: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAuUAAAGKCAYAAACrcD/sAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAAWJQAAFiUBSVIk8AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4wLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvpW3flQAAIABJREFUeJzt3XmcJWV18PHf6Z6d2RgW2YQBBdlE\nnBFUjAsYjYkLuMW4vaIxMcZoNDFRiXnVxIgmcfeNxCgSgpooLgmuaBQXEMVBNCCbwrDDsM2+d5/3\nj6oO16bv7ed29+2avvy+n0996t5bp546XV3Tc/rpp+qJzESSJElScwaaTkCSJEl6oLMolyRJkhpm\nUS5JkiQ1zKJckiRJaphFuSRJktQwi3JJkiSpYRblkiRJUsMsyiVJkqSGWZRLkiRJDbMolyRJkhpm\nUS5JkiQ1zKJckiRJaphFuSRJktQwi3JJknZxEZH1srzpXB4omjrnkzluRJxV7/v20nYj4tT68wsm\nlrGmikW5JEnTJCIWRMSrI+K8iLgxIjZHxKaIuD4izo2Il0TE/KbznC4RsbqlWBxZhiLi7oj4fkS8\nISIWNJ3nA1VdsL89Io5tOpcHgllNJyBJ0gNBRDwT+BiwT8vHm4BhYHm9PBd4T0S8NDO/Pd05NmgT\nsLF+PQdYBvxGvbwyIk7MzDVNJTeD3AZcDdzVxT7r6n1uHGPbqcATgdXAZZPMTeOwp1ySpB6LiFOB\nL1EV5FcDLwX2zMyFmbkYWAo8D7gA2A94QjOZNuYfM3OfelkG7An8HZDAkVS/zGgcmfmWzDw8Mz/S\nxT5frPf5P73MTeOzKJckqYci4hjgDKr/c78KPDIzz8nMu0diMnNdZn4+M08EXgBsaCbbXUNm3p2Z\nbwU+WX90ckTs12ROUq9ZlEuS1Ft/B8wFbgFelJlbOgVn5meB95U0HBGDEXFiRHwwIlZFxB0RsT0i\nbo2IL0bESR32HajHDH+nHsO9IyLujIgrIuLMiHjaGPscHBEfjYhrImJLPSb+hoi4ICLeEhF7luTd\nhc+0vF7Rksf/3tAYEXMj4q8i4ucRsaH+fOmovE+MiC9ExO31+bl9vPMzav+jI+Lf6/22RsRVEfHX\nETG3TfzCiHh+RHwqIi6PiLX1+fplRHwsIg7t0XHb3ujZ4Rj3u9Fz5DOqoSsAnxw17n91HXdm/f7c\ncY7xjjruotK8HogcUy5JUo9ExP7A0+u3H8rMdSX7ZWYWHuIIoHXs+TZgO7AvcApwSkT8VWa+a4x9\n/w14Ucv7dcBiqqEjR9bL10c2RsQKquE1i+qPdlCNBT+wXp4I/LR1nylwS8vrxWNsnwd8Dzi+zmfz\n6ICIeCfwV/XbpPo69+a+8/PuzHxLhxxOoBo+sxuwHgjgYcDfAL8TEU/JzI2j9jkV+HDL+w1UHaEP\nqZcXRcQpmfmtKT7uVNkC3EE1tn92ffzWXybvrNcfB14OPDMi9mj968+IiAjgZfXbM3uUb1+wp1yS\npN55ElUxBfBfPWh/O/A54JlU49XnZ+ZC4EHAXwNDwDsj4tGtO0XEE6gK8mHgDcDizFxKVeTuR1VU\n/mDUsf6RqiD/EbAiM+dk5u5UReNxwAeoCt6pdGDL67VjbH8NcBjwe8DC+mtYTvXLAhHxe9xXkH8E\n2LvOeS/uK5rfHBEv6ZDDPwG/AI7JzCVU5+DlVEXqYxj7rxp31+2fACyt7xuYR/VL1KeoztmnI2K3\nKT7ulMjM/8jMfYCRnu0/bRnzv09mHlfHXVTnOAd4cZvmngwcRPU9+Y9e5dwPLMolSeqdI+r1Nqob\nPKdUZl6Tmb+bmV/OzDtGetgzc01mvhN4B9UvBX80atfH1OvzM/MDmbmh3i8z87bM/NfMfGObff40\nM3/aksPmzPxJZr4hM384xV/iH4wcBrhkjO0LgRfUReT2Op8bMnNH3UP7t3Xcv2fmazPzrjrm7sx8\nHfcNj3lnRLSribYBT8vM/6n33Z6ZZwF/XG///Yg4qHWHzPxMZr4uM3848teR+txeRXWT77eofjF4\nXoevvevjNuTj9frlbba/ol6fO3KdaWwW5ZIk9c4e9freLoakTKXz6vXjRn2+vl7v3aEYHW1kn30n\nnVUHETEnIo6MiI9TPSISqqL6zjHCf56Z57dp6ljgofXrd7aJeUe9PohqCMxYzsjMe8b4/GzgZqpa\n6tlt9r2f+jr4Sv129PelZ8ftobOp/mJzbEQ8snVDRCzhvhwdujIOi3JJkmawiJhfT7JzQUSsqW/Y\nzPpGvZEe7dFPLvkWVSG1ArggqkmLxnu6yVfr9dkR8e6IeExEzJ6iL+NtLTlvA64Afr/edjH39Q6P\n1qlnfuTG0Dsz84qxAjLzau4bt75irBiqcfRj7TsMfL/dvhFxQES8p74Bd21UkyKNfI3vr8M6nfMJ\nHXe61ePIv1S/Hd1b/iKqYTvXZub3pjWxGciiXJKk3hm58W33ejjFlIqIfakmdXkf1Y2We1EVtXdS\n3ag3MonMr41dzsxfAq+mGp/8eKqbPm+JambRj47u8az9BdUY40XAm6gK4vUR8e2oZimdzEykm+p8\n7wBuBa4EvkA11OPxmTnWeHK474bDsexVr2/pEANVr3Nr/Gid9h/Z9mv7RsQTqb6Gv6QqnJdQ3ew5\n8jWO/NWh05jyro/boJEhLC+KiDktn48MXfkkGpdFuSRJvXNlvZ5L9eSMqfYBqhsdr6Ma6rGsnpBo\n7/pGvce02zEzzwQOBl4P/CfVLxDLqcafr4qI00bF3001w+ZTgA9R9cLPAU6kuinx8og4YIJfR+vk\nQftn5pGZ+dz6ee47O+w3VND2mI8PnCL3+0Wr/uvBOVTj3b9FNRHU/MxcOvI1An/Wbv+JHrdh3wKu\npxqu9SyAiDgKeBTV9+hfm0tt5rAolySpd75LdZMi1MXKVKl7JE+u3744M7+QmfeOCntQpzbqm0M/\nmJmnUPW6Hg98karo+9uoJj5qjc/M/FZm/mlmrqB6fOKrgHuAQ7hvWMauYKQX/cCOUTDyi0S7XvdO\nQ0xGxte37vvYus17gJMz8/uZuXXUfh2/LxM8bmPqcfIjY8ZHhrCMDD/6RmbeOv1ZzTwW5ZIk9Uhm\n3sx9Y7FfGxFjPWv7fgqHuuzJfb3AP20T85slx4P/LbgvAZ7PfTcS/sY4+9ybmR8DRnrVn9gpfppd\nWq93i4gxb+KMiMOA/UfFjzbm11R/jx4/xr4jRf41mXm/56bXSr4v3R63F4ZHDlsQ+0mqXvHfqp8K\nM/KYSW/wLGRRLklSb72Vapz3AVTPpp7XKTgifpf7hjd0sp77euEfPkY7+wKvbXOMOWN9DpCZQ1QT\n8UBd9Ec1+2enCQe3tMbvIi4Dflm/Pq1NzNvr9Wrgx21iXh2jZgitvQR4MFXh+oWWz0ee1X7oWN/r\niHgq1ZCf8XR73F4YGfs+Vh6/JjNvAb4GDFI9i30vqp78Xjyfvy9ZlEuS1EOZeRnVJDdJNbvnT+un\nnSwbiYmIJRHxnIj4DtUEK4vGbu3X2t1I9WQSgDMj4ti6rYGIeDLV0Jl2PZzviohzI+KUUXk8KCI+\nRDXWPIFv1psWA7+Majr7h0fE4Khj/V0d943xz8j0qIdUvLV+e3JEfDgi9gCIiD3qr/OF9fa31k81\nGcs84OsRcXS97+yIeBlwRr39E5l5Y0v8hVQzi+5B9aSafev95kfEK4DPc98NwJ10e9xeGHlqzXPq\nxxuOZ+SGz5FHPZ6TmTvaBWuUzHRxcXFxcXHp8UI1rfsdVMXuyLKB+3q8R5bVwBNG7Tuybfmozx9N\nVQCObN/Y8v5uqjHnSV2jtuz3gVHHXDdGHqe1xC8dtW173f7Ols9+BRzQ5TlZXe/79i73O6t0P6pn\nlI/kOEQ11nuo5bPT2+w3sv1FVE+HSapZRbe1bPsh1Uyio/d93ajztZbqrw8jj6l8bf36gik+btvz\n0uEaOrVDLoe3HHcH1VNfVgM/aHPOZlE9PWfkWEc3/e9uJi32lEuSNA0y80tUN0O+hmqc+c1URcws\nqkLnXKpC7GFZ+EznzPwR1Y2FXwLuBWYDa4B/ppo852dtdn0/VeH4n8A1VD3qc4GbqHrqn5CZ72qJ\nXw88g6qY/zHVsIRFVEXjJVRT2R+b1Rj6XUpmvpVqqvf/pHpE5EKqXyj+C/jNzHzLOE1cRPXLz2e5\nr0C9Gvi/wJOy+ovF6GN+CHgO9/WazwKuAt4GnED1y9h4uj7uVMtqBtKnAF+n+sVtH6qJlsZ8yk5W\nT8oZmbDqksy8vNc59pOof7ORJEmSJiUirgEOBV6dmWeMF6/7WJRLkiRp0ur7C75F9ReU/TJz/Ti7\nqIXDVyRJkjQpEbEn8A/12zMtyLtnT7kkSZImJCL+EfhdqvHms6nG7R+VmWsaTWwGsqdckiRJE7Un\n1XPTtwDnAydZkE+MPeWSJElSw+wplyRJkhpmUS5JkiQ1zKJckiRJatisphPolacMPN/B8pJmrG8O\nfy6azkGSNH3sKZckSZIa1rc95ZKkmSEirgcWA6sbTkWSJmI5sD4zD55MIxblkqSmLZ4/f/6yI444\nYlnTiUhSt6688kq2bNky6XYsyiVJTVt9xBFHLFu1alXTeUhS11auXMmll166erLtOKZckiRJaphF\nuSRJktQwi3JJkiSpYRblkiRJUsMsyiVJkqSGWZRLkiRJDbMolyRJkhpmUS5JkiQ1zKJckiRJaphF\nuSRJktQwi3JJkiSpYRblkiRJUsNmNZ2AJEmX37KO5W/+StNpSOoTq9/99KZT6Jo95ZIkSVLDLMol\nSZKkhlmUS5I6isqLI+K/I+LuiNgaEddHxBkRcXDT+UlSP7AolyS1FRGzgS8C5wAnAYuBzcBy4FXA\nzyPipMYSlKQ+YVEuSerkPcDJwE7gDcCSzFwGPBj4HLAQ+EJE7NtcipI081mUS5LGFBF7A6+p374v\nMz+QmZsBMvNm4IXAlcAS4K3NZClJ/cGiXJLUzknAnPr1+0dvzMwh4EP12xfWQ10kSRNgUS5Jaueg\ner0uM29vE3NVvd4dWNH7lCSpP1mUS5LayXrd6f+K1knojuphLpLU15zRU5LUzg31elFEPDgzbxoj\n5siW1/t1aiwiVrXZdPhEkpOkfmJPuSSpne8A2+vXbxq9MSLmAK9v+WjRdCQlSf3InnJJ0pgyc01E\nnAG8DvjjiFgHfBS4Azga+AfgYGAHMBsYHqe9lWN9XvegOx5d0gOaPeWSpE7+EjgPCOA04Caq3vNL\ngScD/w+4ro5d20SCktQP7CmXJLWVmdsi4mTgecCLqW7mHKR66sq/UBXs6+vwaxtJUpL6gEW5JKmj\nzEyq2Ts/N3pbRBwPzK/fXjydeUlSP7EoV/8bGCyPHR4qCou5c4ubvO7t5UNl590VxbEbjto+flBt\nt6VbimMHBjoOC/4127eX/wjZds/88YNqs+8p/57tfmVZ3II1O4rbnHfLxuLY4cuvGj+ov728Xl+Q\nmbc2mokkzWCOKZckTUhEPBZ4Zf329CZzkaSZzqJcktRWRJwYEW+IiEMiYrD+bPeIeC3wDaq/uH4s\nM89vNFFJmuEcviJJ6uQg4H31sjMiNgJLqJ7GAvBx4I8byk2S+oZFuSSpkx8AHwSeABxINUHQzcCF\nVD3k32kwN0nqGxblkqS2MvOX/PqsnZKkHrAolyQ17uj9l7Dq3U9vOg1Jaow3ekqSJEkNsyiXJEmS\nGmZRLkmSJDXMolySJElqmDd6ShNwzwtXFMde87KPFsdesX1LcexRc8qnrRfcvHNjceyigcHi2Md9\n5M+LY/d/90XFsZKkBxZ7yiVJkqSGWZRLkiRJDbMolyR1FBEDEfHyiPhWRNwZETsiYm1E/Cgi/ioi\nFjWdoyTNdI4plyS1FRELgPOAk1o+Xg8sBo6vlz+IiJMy87oGUpSkvmBPuSSpk7+mKsgTOA1YmplL\ngHnAC4G1wEHAxxvLUJL6gD3lkqROXlSvP5mZp498mJnbgX+PiHnAJ4ETI2L3zLy3iSQlaaazp1yS\n1MmD6vVP22xf1fJ6QY9zkaS+ZVEuSepkdb1+ZJvtK+v1HcCtPc9GkvqURbkkqZN/qdcvj4g3R8QS\ngIiYExEvAN5PNd78jZmZTSUpSTOdY8rV/3J4ypu8+5jy2uNXO8pnkrxk6yHFsTftXFccu2Zn+RPr\n9plV3m43hrvoA9g0PLc4do/BsvN799B+xW3+1oI1xbH7XLy1OHaG+gBwMPAa4HTg9IhYByyi6ti5\nGPi7zPxycylK0sxnUS5JaiszhyLi9cB1wHuo/t9Y0hKyCNirpK2IWNVm0+GTSlKS+oDDVyRJbUXE\nPsCFwHuBTwGPABYChwJvAQ4BzoyI09s2Ikkalz3lkqROzqaaIOgTmfnKls9/Cbw7Im6pY/4yIj6V\nmZe3aygzV471ed2DvmIKc5akGceecknSmCLiSOAp9dv3jxWTmf8G3E31/8kzpik1Seo7FuWSpHaO\naHl9fYe46+r18t6lIkn9zaJcktRO66OLDuwQd1C93tDDXCSpr1mUS5Lauazl9R+MFRARzwT2rt/+\nqOcZSVKfsiiXJI0pM68Hzq/fvj4iTo+IvQEiYmFEnAqcVW9fDfzXdOcoSf3ColyS1MmpwJVU/1+8\nGbgjItZTDVX5JLAMuAN4TmZubypJSZrpLMolSW1l5m3ASuD1wPeAe4AFwHrgUuBvgYdn5k8bS1KS\n+oDPKVf/y5zyJn/r8ZeNH1QbIopjHzz77uLYebGjOPZh88vbnV2eLp9df0xx7PMW/7w49sBZC4tj\nr9+xsShu2eDm4jbvGd5ZHDvnlrXFsUPFkbuWzNwCfLBeJEk9YE+5JEmS1DCLckmSJKlhFuWSJElS\nwyzKJUmSpIZZlEuSJEkNsyiXJEmSGmZRLkmSJDXMolySJElqmEW5JEmS1DCLcknSmCIiu1ie2HS+\nkjSTzWo6Ael/Rfn87jFrdnFs7tg+kWw6+qf9Ly6O/frmZcWxa3YuKo7dbaD86/ra+kcUxx48987i\n2EPmrimOfeONJxfHHrv45uLYJbM2F8WtnLe6uM29BnYWxzJrsDx25rljnO2LgfnAduDy3qcjSf3L\nolySNKbM3KfT9oi4DHgE8OXMvHt6spKk/uTwFUlS1yLiWKqCHOBfm8xFkvqBRbkkaSJeVq/vBL7a\nZCKS1A8syiVJXYmIWcCL6refyswuBuFLksZiUS5J6tZvA3vXrx26IklTwBs9JUndOrVe/zwzLyvd\nKSJWtdl0+KQzkqQZzp5ySVKxiFgGPKN+e1aDqUhSX7GnXJLUjRcCc4CdwKe62TEzV471ed2DvmLy\nqUnSzGVPuSSpGyNPXflaZpbPHiVJ6siiXJJUJCKOAI6r33qDpyRNIYevaNeR2XQGDB72kMLI4nvb\nWDu0oDj2qLm3FscOUn6+Lhp6aHHsbgPbimNPmDfeLOz3Wbvn/xTHnnv7mKMcxnTIwruK4g6dc3tx\nm/Oi/Bww8IDq2zi1Xt8DnNdgHpLUdx5Q/5tIkiYmIgaAl9RvP5OZ25vMR5L6jUW5JKnEU4D96tcO\nXZGkKWZRLkkqMXKD5y8y85JGM5GkPmRRLknqKCIWA6fUb+0ll6QesCiXJI3nd4H5wDBwTsO5SFJf\nsiiXJHWUmR/PzMjMwcwsf0SQJKmYRbkkSZLUMItySZIkqWEW5ZIkSVLDLMolSZKkhs1qOgFpInJH\nbyYT/NX/2bso7srtm4vb3JplbQLMi6Hi2O9vfmhx7MV3Li+Ofe++lxbHXrG9/Pf6UxevKY49bt4X\nimO/uP6RRXGLB7YWtzkQURx725P2KI7d+4riUEnSA4w95ZIkSVLDLMolSZKkhlmUS5IkSQ2zKJck\nSZIaZlEuSSoSEYdExPsj4sqI2BgR6+rXZ0bEE5vOT5JmMp++IkkaV0S8AvgIML/+aBMwGzi8XoaB\n7zaTnSTNfPaUS5I6iojfAz5OVZB/BHhIZi7MzAXAPsBLgYsaTFGSZjx7yiVJbUXE3sA/AQGclpmn\nt27PzDuAc5rITZL6iT3lkqROXg3sDlwNvKfhXCSpb1mUS5I6eXG9PjszhxvNRJL6mMNXpBZved7n\ni+LuHF5Q3OZQlv/u+6DB8prn2i0PKo5d+919imOvOWJTcezWnF0c241nfPu1xbGLl5Xl+9bjrypu\n85od5d+Hp76yfCj1Zf+vOHSXEBF7AIfWb38QEScBbwKOB+YCq4H/Av4xM+9qJElJ6hMW5ZKkdg5t\nef1U4DSqseUb6s+OqJeXRMRTMvPKTo1FxKo2mw6fbKKSNNM5fEWS1M7SltenAVcAj87MxcBC4HeA\nNcD+wOcjwo4eSZogf4BKktpp7bgZAp6dmb8EqMeXf61+fvmXqXrMnw18rl1jmblyrM/rHvQVU5W0\nJM1E9pRLktrZ2PL6KyMFeavM/ApwTf32N6clK0nqQxblkqR2bm15fXWHuJFtD+5hLpLU1yzKJUnt\nXAdsqV9nQXxJjCRpDBblkqQx1ePGL6jfdnpCysPq9Q09TUiS+phFuSSpk3+r10+PiIeO3hgRTwcO\nq99+ddqykqQ+Y1EuSerkP4BVVE/r+mJEHAcQEQMR8TTgE3Xcj4GvNJOiJM18PhJRktRWZg5HxCnA\nd4GjgR9HxAZgEBiZ2vZq4HmZ6ZhySZogi3LtOgYGy2OHh4pDB488bPyg2qmLLyuKO3v9nsVt7jFr\n4/hBtfLJ3eGERfd7Ol1b//PdhxfHvvfZ5U+1O/+yo4tjD/3XHcWxR9y7vjj20Z+5vChu4/DW4jZv\n3bmoOPY9Dyq7ZgB+i2OLY3clmXlzRDwCeCPwXOAQqps6fwqcC3woM8svdEnS/ViUS5LGVRfdb68X\nSdIUc0y5JEmS1DCLckmSJKlhFuWSJElSwyzKJUmSpIZZlEuSJEkNsyiXJEmSGmZRLkmSJDXMolyS\nJElqmEW5JKmtiDg1InKcxdk8JWmSnNFTu4wYiOLY7GI++l+9bV5x7G07y2qLrblfcZuHDd5RHPuL\n7eXTu58w79bi2Bs+9v3i2P1m31scu8+j1xfHnj38uOLYmDdYHPvVvX5RFHfh1jnFba7esWdxLPPX\nFIfe9mcnlLe769kB3NNm26bpTESS+pFFuSSpxEWZ+aSmk5CkfuXwFUmSJKlhFuWSJElSwyzKJUmS\npIZZlEuSShwVEVdExJaI2BARl0fE+yPi4KYTk6R+YFEuSSqxJ3AEsBmYBxwFvB64IiJe1GRiktQP\nfPqKJKmTW4G3AZ8Hrs3M7RExF3gy8A/AkcDZEXFzZn6vU0MRsarNpsOnMmFJmoksyiVJbWXm+cD5\noz7bBnw1Ii4EfgI8FHg3MKMfxC5JTbIolyRNSGaui4h3AWcCj4mIvTLzzg7xK8f6vO5BX9GjNCVp\nRnBMuSRpMn5UrwNY3mAekjSj2VOuXUbu3NmTdq9+/NnFsRdsWVwUt3x2287A+9lO+ZTxAzFcHHvt\nzoXFsb+/9Iri2M3DQ8Wxv7fo3uLYdzyrPIdurNq2vShuR84rbnPZ4Mbi2Ot3lMce+dyrimNnkGh5\nnY1lIUkznD3lkqTJOL7l9Q2NZSFJM5xFuSRpTBER42xfDLy5fvvjTuPJJUmdWZRLkto5KCIujojf\nj4gDRz6MiDkR8TTgQuAwYBh4S1NJSlI/cEy5JKmTR9cLEbEV2AQsBmbX2zcDf5SZ324mPUnqDxbl\nkqR27gBeB/wG8AhgL2AJVWF+LfDfwEcz07HkkjRJFuWSpDFl5hbgw/UiSeohx5RLkiRJDbMolyRJ\nkhpmUS5JkiQ1zDHlmpFW/8cxxbE37/xBceyaof2L4vafVT6T5Y4s/2c2O8pnNR3sYvLEn22fXxy7\ndmhBcey87TuKY+dE+Uyh64fLZ9+cE3OL4pYObC5uc/XQnsWx9wyvLY59zl6rimMlSQ8s9pRLkiRJ\nDbMolyRJkhpmUS5JkiQ1zKJckiRJaphFuSRJktQwi3JJUrGIWBgRN0VE1supTeckSf3AolyS1I13\nAgc0nYQk9RuLcklSkYhYAfwJ8KOmc5GkfmNRLkkaV0QMAP9cv311k7lIUj+yKJcklXgt8Cjgo5n5\n06aTkaR+Uz7/t9Rjs/bfrzj26sefXRz72Y37F8eWTgW/drh8Kvpupnef3cVU9DtysDh2DuXtLh7Y\nWhy71+Cm4thN2ZsfNwMxXBS3ZmhRcZvbu8i1m+/DbgPbimN3JRGxP/C3wB3AWxtOR5L6kkW5JGk8\nHwYWAX+cmeuaTkaS+pFFuSSprYh4JvBs4ILMPGeSba1qs+nwybQrSf3AMeWSpDFFxG7AR4AdwGsa\nTkeS+po95ZKkdv4GOBD4+8z8xWQby8yVY31e96CvmGz7kjST2VMuSbqfiDgW+FPgJqriXJLUQ/aU\nS5LG8kFgEPgrICJiYZu4ufW24cwsf9SQJOnX2FMuSRrLQfX6bGDDGMuIM+r3kx7eIkkPZBblkiRJ\nUsMsyiVJ95OZyzMz2i0toS+vP1veVK6S1A8syiVJkqSGeaPnLiJmlX0rcufOHmfSnNmfLp8K/srt\n5feTbR7euzh28ayyyQoXdDFd+uwo/7qGs/z35HlRfi2sHZ5fHLtbbC+O3drFFPPdGIjh4tjbdy4p\nitt7cMP4QSNmrS0Ofcy88nNw8JdfWhz7zEOKQyVJfcCeckmSJKlh9pRLkro2aly5JGmS7CmXJEmS\nGmZRLkmSJDXMolySJElqmEW5JEmS1DCLckmSJKlhFuWSJElSwyzKJUmSpIZZlEuSJEkNc/KgXUTu\nLJwyPXo0X0dmT5pd+9LHFsd+49CPFseevf7A4tiHz725OHbD8LyiuHmxo7jN4Sz/3XeILr6/XUxx\nv8/gxuLYu4fnF8euHyo7XwCDMVwcu9/ghuLYFQu2FsXdsLP8Gv/R5ocUx163vfxaOOwPLymO5ZXl\noZKkmc+iXJLUVkQ8CjgZOA54KLAXMA+4C/gJ8MnM/FJzGUpSf7AolyR18krgVS3vNwLDwH7As4Bn\nRcTngRdmZvmfDSRJv8Yx5ZKkTn4IvAFYCSzKzEWZOR84EPiHOua5wJsbyk+S+oI95ZKktjLzX9t8\nfhPwlxGxL/AS4FTgb6cxNUnqK/aUS5ImY+Tu1f0azUKSZjiLcknSZJxQr69vNAtJmuEcviJJ6kpE\nLAQOoboB9AX1xx9pLiNJmvnKysJpAAAUPklEQVQsyiVJ44qIA4Cbxti0FXhXZv5TQRur2mw6fDK5\nSVI/sCiXJJUYAu6oX+8OzAF2AqdjL7kkTZpFuSRpXJl5G7APQEQMUE0k9CbgHcDvR8TvZOYV47Sx\ncqzP6x70FVObsSTNLBblQMwqPw3dxObOnRNJp/Px58wpjh3evHnKjw8weNTDimN/9J6PFsd+r2y2\ndADmDZTPUXLTzmXl7UZZu7Oz/Hs71KP7qYe6iL1p59Li2NJzAHDs3LXFsbcPDRbHfmvTEcWxl6w/\nqCju8jv3LW7zyD3vGD+odsaBXy+O/RqPKY7dlWXmMHANVTG+Fvgz4JyIWFlvkyR1yaevSJIm48P1\n+ljgkU0mIkkzmUW5JGkybml5/ZDGspCkGc6iXJI0GQe3vN7YWBaSNMNZlEuSxhQRgxER44T9Rb3e\nCfywxylJUt+yKJcktfNg4CcR8Yr6OeVA9fSViDg2Ij4FvLL++MOZeW8jWUpSH/DpK5KkTlYAnwCI\niK1UQ1QWAXNbYs4C/nLaM5OkPmJRLklq51bgBcCTgeOBfYE9qGbx/BXVcJVPZuaFjWUoSX3ColyS\nNKbM3A58tl4kST3kmHJJkiSpYRblkiRJUsMcvgLkzvIp07uJ7YVeHf/2N5xQHPuzv/in4tg1Q5uK\nY6/dVj7vyPI5dxbHbhieXxy7aGBLUdxQF7/PDlI+6/iOLP8nuWhga3FsN79+71F4DgA+es9xxbFf\nu+XI4thDltxdHPvYpdcVxb1p328Ut3lAFz8Zf7nDvg1J0uT5v4kkSZLUMItySZIkqWEW5ZIkSVLD\nLMolSZKkhlmUS5IkSQ2zKJcktRURB0bE6yPivIi4MSK2RcSGiPhZRLw7IvZtOkdJ6gc+ElGSNKaI\neDCwGoiWj9cDuwHH1MsfRsRzM/M705+hJPUPe8olSe0M1uuvAM8HlmXmEmAB8DvA9cDuwJciYp9m\nUpSk/mBRLklq517gkZn5jMw8NzPvBcjM7Zn5NarCfCuwGHhVg3lK0oxnUS5JGlNmrsvMn3XYfhVw\ncf125fRkJUn9yTHlXdr8nEcXx9718MHxg2o7d8uiuIGDyqet//TxHy+OXTn3suLYVdu2F8fetPNB\nxbFHzb2lOHY75ed22eDG4titObsorpsp7ndkN7luLo69Z2hBcezxc8vzfc/djyqOvX3b4uLYjx15\nTnHsQbPK/j0AzC68Fn64bWFxmw8aXF8c++xvvq449jAuKY6dQe6u1+UXuiTpfuwplyRNSETMAh5X\nv728yVwkaaazKJckTdRrgH2AYeDshnORpBnN4SuSpK5FxDHAu+q3H8nMKwr2WdVm0+FTlpgkzVD2\nlEuSulJPGPQlqkcjrgLe1GxGkjTz2VMuSSoWEcuA84GDgWuBp2dm0Z3EmTnmE1rqHvQVU5akJM1A\n9pRLkopExBLgG8DRwI3Ab2bmHc1mJUn9waJckjSuiNgN+CrwKOB2qoL8xmazkqT+YVEuSeooIuYD\n5wEnUD2X/Dcz89pms5Kk/mJRLklqKyLmAF8ATgTWAk8tedKKJKk7FuWSpDFFxCDwaeBpwAbgtzPz\n0mazkqT+1LdPX1n3kscUx/71288qjn34nB8Ux24YLp91el4MFcVt7WLK9ruH5xfHfmlT+RTkAwwX\nx+43697i2NIp7gHmUHa+AAaiPN/dYntR3CDl08AT5aE37VxaHPv0BUUPvADgw/ceWhx7yZP2Lo59\n/SXl/x6OmTOvOPbrm+cWx+41uKE4ttS8KP93tt83+7pv43HAc+vXs4EvRbS9oG/KzOOmJStJ6kN9\nW5RLkiat9TeOefXSTvlviZKk+7EolySNKTMvoKu/9UiSJqqv/+4qSZIkzQQW5ZIkSVLDLMolSZKk\nhlmUS5IkSQ2zKJckSZIaZlEuSZIkNcyiXJIkSWqYRbkkSZLUsL6dPOjv/+aM4tjSqdUBvrnpocWx\n8wZ2FMcOFk5d382U8YsHyifY22dwXXHsvNhZHLu9i9/7uvk+9MrSgbIcbhlaWNzmHgNbimOPX1D+\nPTviwpcWxx74/P8pjoV7iyOfuqD8Gv/vLeVT1x80qzyHdcNzi+LmRXmudw6VX+NLf3xrcWx5q5Kk\nB5q+LcolSZMXEYuAE4HjgEfV6z3qzUdk5lVN5SZJ/cSiXJLUyZOBLzadhCT1O4tySdJ41gA/AS4B\nbgE+1mw6ktR/LMolSZ2cl5lfGnkTEcubS0WS+pdPX5EktZWZQ03nIEkPBBblkiRJUsMsyiVJkqSG\nOaZckjQtImJVm02HT2sikrQLsqdckiRJapg95ZKkaZGZK8f6vO5BXzHN6UjSLqVvi/JXfOGPimPP\nfe4Hi2Oft+j6iaQzrnXDZQ842DRc/seNO7qYCn7N0KLi2N0GthXHDmV5vnOi/CEPQ0RxbDduH1pc\nFHfC3HuK29x9cEFx7G+87lXFsQee+6Pi2G4M/8axXURfVhzZzTT39wzPK47dNDy3KG7xwNbiNn+x\nY8/i2J033FQcK0lSOw5fkSRJkhpmUS5JkiQ1zKJckiRJaphFuSRJktSwvr3RU5I0NSKi9c7X3Vte\nLx217Z7MHJ6mtCSpr1iUS5LGc2ebz3846v3BwOrepiJJ/cnhK5IkSVLD7CmXJHWUmb2ZGECS9L/s\nKZckSZIa1rc95Q9548XFsW9520nFsXf+3jHFsQtfcFtx7GmHfKUo7qkLymdFPIJu7rfa3EVsN8pn\n6ezGjixvd3YMdtFyWbs/3ja7uMV3PP5ZxbG7re7NLJ3d2HBQ+Wya3ZjdxYytx88tP79Q9m/i3qHy\nmWh3Hyy/Zj5UHClJUnv2lEuSJEkNsyiXJEmSGmZRLkmSJDXMolySJElqmEW5JEmS1DCLckmSJKlh\nFuWSJElSwyzKJUnjioh9IuKDEfGriNgaEXdExHkR8eSmc5OkfmBRLknqKCKOAS4HXgccAmwD9gSe\nAXwzIt7cYHqS1BcsyiVJbUXEfOC/gD2AnwJHZ+YSYHfgvUAAp0fEU5vLUpJmvllNJ7ArGN60qTh2\nj0/8sLzhT5SHvpejCuO68JhjikPvesRuxbHrDsvi2HxQ+dTm++65rjh2t9nbi2N3ZvnvnmvOP6Ao\nbr+/v6i4TbixODJmlf+TzKHyaevJ8u/ZsvN+URz7rjc9rDj2K7eWXeMAQ8Pl37N1m+YXxc2evbO4\nzU2rlxTHPpSLi2NnqFcBBwEbgWdm5i0AmbkeeGNEPAQ4BTgdOL+xLCVphrOnXJLUyYvr9adHCvJR\n/qFer4iIw6cpJ0nqOxblkqQxRcQiYGX99httwi4GRv7MdVLPk5KkPmVRLklq5wiqMeMAV4wVkJnD\nwNX12yOnIylJ6keOKZcktbNvy+tbO8SNbNu3QwwRsarNJoe9SHrAs6dcktRO6x3gWzrEba7XC3uY\niyT1NXvKJUntxPgh5TJz5Vif1z3oK6byWJI009hTLklqZ2PL607PnlwwRrwkqQsW5ZKkdlrHke/X\nIW5k2209zEWS+ppFuSSpnauAkZmnxpz9KSIGgJFZpMpnnpIk/RqLcknSmDJzA/CT+u1T2oQ9GhiZ\nAvW/e56UJPUpb/TsZxf/vDh0zy5mCt9zAqk0qZuLfD9u7FkeJXJn+VTwvTK0fn1x7HePKZviHmAh\n100knXEtGT9Ek/Np4DjgxRHxN5k5eojKG+v1qsy8GknShNhTLknq5J+BG4BFwJcj4kioZvuMiL8H\nnlPHndZQfpLUF+wplyS1lZlbIuJkqqEpK4ArImI91TPJB6jGnJ+Wmec3mKYkzXj2lEuSOsrMnwFH\nAx8CrgPmAncDXwGekpnvbjA9SeoL9pRLksaVmbcDf1ovkqQpZk+5JEmS1DCLckmSJKlhFuWSJElS\nwyzKJUmSpIZZlEuSJEkNsyiXJEmSGmZRLkmSJDXMolySJElqmEW5JEmS1DCLckmSJKlhFuWSJElS\nwyzKJUmSpIZZlEuSJEkNm9V0ApKkB7zlV155JStXrmw6D0nq2pVXXgmwfLLtWJRLkpq2cMuWLUOX\nXnrpz5pOZBdyeL2+qtEsdj2el/vznNzfdJ+T5cD6yTZiUS5JatrlAJlpV3ktIlaB52Q0z8v9eU7u\nb6aeE8eUS5IkSQ3r257ybw5/LprOQZIkSSphT7kkSZLUMItySZIkqWEW5ZIkSVLDIjObzkGSJEl6\nQLOnXJIkSWqYRbkkSZLUMItySZIkqWEW5ZIkSVLDLMolSZKkhlmUS5IkSQ2zKJckSZIaZlEuSZqQ\niDggIs6MiFsjYltErI6ID0TE7l22s6zeb3Xdzq11uwf0+thTbbJ5RcRuEfHiiPh0RFwVEZsiYkNE\n/CQi/jwi5rTZLzssF0/tV9m9qfh+RcQF43yd89rsd2REfDYi1kTE1oi4OiLeERHzp+4r7N4UXCtP\nGud8jCwPHrXfLnmtRMTzIuLDEfH9iFhf53POBNvq+tzuCteJkwdJkroWEQ8BLgL2Bv4TuAo4HjgR\nuBp4XGbeXdDOHnU7hwHfBi4BDgdOBtYAj83M63px7Kk2FXlFxNOArwH3AN8BfgksA54J7FO3/+TM\n3DpqvwRuAM4ao9mbM/PjE/7CJmkKr5ULgCcC72gT8s7M3Dlqn0dTXVezgXOBm4CTgEcBF1Kdy23d\nf1WTM0XXynLg1DabHw48B7giM48etd8uea1ExGXAI4CNwM1UPwc+lZkv6bKdrs/tLnOdZKaLi4uL\ni0tXC/ANIIHXjvr8ffXnZxS28891/PtGff66+vOv9+rYu+I5AY4FXgzMGfX5ImBV3c6fj7FfAhc0\nfV30+Fq5oCpbio87CPyiPsazWj4foCq8EnjzTD4nHdr/TN3O62bKtUJVNB8KBPCkOs9zen1ud6Xr\nxJ5ySVJXIuIQ4FfAauAhmTncsm0RcBvVf6x7Z+amDu3sBtwJDAP7ZuaGlm0D9TGW18e4biqPPdWm\nI6+IeBHwKeDLmfnMUdsS+G5mPmlCX0CPTOV5Gekpz8woPPZJwH8D38vMJ7bJ6wbg4JzGYqjX10r9\n16dbqP5d7Z+Z947avkteK60i4klUfynqqqd8Iud2V7pOHFMuSerWSfX6/Nb/9ADqwvpCYAHwmHHa\neSwwH7iwtSCv2xkGzq/fntiDY0+16chrR73e2Wb70oh4RUScFhGviYjpPgdjmfLzEhEviIg3R8Sf\nRcRvR8TccY799dEb6l/yrgEOAg4pPfYU6fW1ciowF/jc6IK8xa54rUyFiZzbXeY6sSiXJHXrYfX6\nmjbbr63Xh/Wgnak69lSbjrxeUa/vVzzUHgF8Avg74CPADyPisoh4+CSOOVm9OC//DpwOvBf4KnBj\nRDxvmo49FXqd1yvr9T93iNkVr5WpMKN/pliUS5K6taRer2uzfeTzpT1oZ6qOPdV6mldE/AnwNOAy\n4MwxQt4HPA7Yi2r8+XFU42EfAXw7IvafyHGnwFSel/+kuuH1AKq/sBxOVZwvBf4jIn67h8eeSj3L\nKyKeSHVersjMi9qE7arXylSY0T9TLMolSVNtZMzvZMdfTqSdqTr2VJtwXhHxHOADwO3AczNzx+iY\nzPzzzLwoM+/KzI2Z+ZPMfD7weWBP4I2TyL2Xis9LZr4/M7+cmbdk5tbMvDozTwP+nKqeeVevjj3N\nJpPXH9brtr3kM/hamQq79M8Ui3JJUrdGeo6WtNm+eFTcVLYzVceeaj3JKyJOoRqusQZ4Uo56PGSB\nM+r1E7rcb6pMx/fr41Tj7I+tb+abzmNPRK+ulWXAc4EtwL9NIK+mr5WpMKN/pliUS5K6dXW9bjfG\n8tB63W6M5mTamapjT7Upzysing98DriD6qkjV4+zy1jurNe7TWDfqdDz71dWz2wfuVG49et8wFwr\ntZdR3eD52cxcO4G8mr5WpsKM/pliUS5J6tZ36vVT60cX/q+6p/JxVL11480OeHEd97hRPZwjj0R8\n6qjjTeWxp9qU5lU//vAzwK1UBfm14+zSzshTJrrtYZ8qPf9+RcTDgN2pCvO7WjZ9u14/bYx9DqEq\nwm5g+s9Nr87JH9Trj00wr6avlakwkXO7y1wnFuWSpK5k5q+oHle4HHjNqM3voOppO7v1GcsRcXhE\nHD6qnY1Uf2bfDXj7qHb+pG7/G61DNiZy7OkwVeek/vxlVOflRuAJ4w1ZiYgV9TPfR39+DNXTNQAm\nNF35ZE3VeYmIQ8a6ATEi9gQ+Wb/99/z1GT2/C1wJPCEintWyzwDwnvrtGdP5jHKY2mulZfvjgSOA\nyzvc4LlLXyvdiIjZ9Tl5SOvnE/z5sMtcJ04eJEnq2hhTWV8JPJrqmeLXACdky1TW9YQljJ74pZ7o\n5CKq3qhvAz+mKi5OphpHfUL9H+2Ejz1dpuKcRMSJwLeoOs3OpJrue7S1mfmBln3OoppS/dt1/Daq\nJ3A8jWq2wn8BXjXdxWdLflNxXk6lGjv+XarJXO4BDgR+h2os8E+Ap4wetjHG9Ok3Ak9muqdPH2Wq\n/v20bP834CVUM3h+uMNxz2IXvVbq+ydOqd/uA/wWVe/09+vP7srMN9axy4HrgRsyc/modrr++bDL\nXCfdTgHq4uLi4uKSmQAPpuqlvA3YTvUn3g8Cy8aITdpMkQ4sq/e7oW7nNqqC9ICpOPZMOidUE7/k\nOMvqUfucAnwB+CWwvuUcnkfLtOEz/Lw8HDgL+B/gbqqJlO6hKtheC8zpcOwjqcbm30VVhF5D1Ws6\nfyafk5Ztu1MNydgMLB3nmLvstUL117Ki656qJ/x+/xYmcm53pevEnnJJkiSpYY4plyRJkhpmUS5J\nkiQ1zKJckiRJaphFuSRJktQwi3JJkiSpYRblkiRJUsMsyiVJkqSGWZRLkiRJDbMolyRJkhpmUS5J\nkiQ1zKJckiRJaphFuSRJktQwi3JJkiSpYRblkiRJUsMsyiVJkqSGWZRLkiRJDbMolyRJkhpmUS5J\nkiQ1zKJckiRJaphFuSRJktSw/w/MTI1yRDnDOwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1207c06d8>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 197,
       "width": 370
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Test out your network!\n",
    "\n",
    "model.eval()\n",
    "\n",
    "dataiter = iter(testloader)\n",
    "images, labels = dataiter.next()\n",
    "img = images[0]\n",
    "# Convert 2D image to 1D vector\n",
    "img = img.resize_(1, 784)\n",
    "\n",
    "# Calculate the class probabilities (softmax) for img\n",
    "output = model.forward(Variable(img, volatile=True))\n",
    "ps = torch.exp(output)\n",
    "\n",
    "# Plot the image and probabilities\n",
    "helper.view_classify(img.resize_(1, 28, 28), ps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Up!\n",
    "\n",
    "In the next part, I'll show you how to save your trained models. In general, you won't want to train a model everytime you need it. Instead, you'll train once, save it, then load the model when you want to train more or use if for inference."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
